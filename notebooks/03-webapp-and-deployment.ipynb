{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "WhatsApp Chat Analysis.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMKNvsa45iV3ZkoOd+f5A6J",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gborn/Whatsapp_Chat_Analysis/blob/main/notebooks/03-webapp-and-deployment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Analysis of WhatsApp Conversations - WebApp and Deployment\n",
        "\n",
        "In this final notebook, we create a streamlit webapp, and deploy it to heroku. All the steps are documented in the notebook.\n",
        "<br>\n",
        "The webapp has been deployed at [chatresults.herokuapp.com](https://chatresults.herokuapp.com/)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "iQmVxhcrJHgW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Streamlit WebApp\n",
        "\n",
        "<br>\n",
        "We create three files preprocessing.py, utils.py, and app.py.<br><br>\n",
        "preprocessing.py reads an exported WhatsApp chats text file, and performs preprocessing steps outlined in data wrangling notebook.<br><br>\n",
        "utils.py has helpful helper functions to filter data and plot visualizations <br> <br>\n",
        "app.py renders a Streamlit web app UI and is the main entry point for our application"
      ],
      "metadata": {
        "id": "MAGHEEy1BpT8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# install all necessary libraries\n",
        "!pip install streamlit > /dev/null\n",
        "!npm install -g localtunnel > /dev/null\n",
        "!pip install fire > /dev/null\n",
        "!pip install urlextract > /dev/null\n",
        "!pip install wordcloud > /dev/null\n",
        "!pip install emoji > /dev/null\n",
        "!pip install altair > /dev/null\n",
        "!pip install numerize > /dev/null\n",
        "!pip install top2vec > /dev/null"
      ],
      "metadata": {
        "id": "o73yVQNmKlM1"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir -p utils"
      ],
      "metadata": {
        "id": "MJtJckI6pyEk"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile utils/preprocessing.py\n",
        "\n",
        "import pandas as pd\n",
        "import re\n",
        "import csv\n",
        "import fire\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "def separator(msg):\n",
        "    \"\"\"\n",
        "    Extract datetime, person name or number, and message text from msg\n",
        "    \"\"\"\n",
        "    # remove whitespaces\n",
        "    msg = msg.strip()\n",
        "\n",
        "    # we define three groups: datetime, person name/phone number, message\n",
        "    result = re.search(r'(\\d{1,2}/\\d{1,2}/\\d{4},\\s+\\d{1,2}:\\d{1,2})\\s+-\\s+([+0-9a-zA-Z\\s]+):\\s+(.*)', msg)\n",
        "\n",
        "    # ignore lines that don't have above three groups, and return empty line\n",
        "    if not hasattr(result, 'group'):\n",
        "        return ''\n",
        "\n",
        "    return result.groups()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def txt_to_csv(fn):\n",
        "    \"\"\"\n",
        "    Read lines from text file \"fn\",\n",
        "    create and save a csv file\n",
        "    \"\"\"\n",
        "    with open(fn) as f:\n",
        "         lines = map(separator, f.readlines())\n",
        "\n",
        "    with open(f'{fn[:-4]}.csv', 'w') as wf:\n",
        "        out = csv.writer(wf)\n",
        "        out.writerow(['datetime', 'id', 'message'])\n",
        "        for line in lines:\n",
        "            if line:\n",
        "                out.writerow(line)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def add_datepart(df, fieldname):\n",
        "    \"\"\"\n",
        "    Adds date related features to dataframe df inplace\n",
        "    df: dataframe\n",
        "    fieldname: name of the date field in df\n",
        "    \"\"\"\n",
        "    new_df = df.copy()\n",
        "    field = df[fieldname]\n",
        "    target_prefix = re.sub('[Dd]atetime$', '', fieldname)\n",
        "    \n",
        "    date_features = (\n",
        "         'hour',\n",
        "         'minute',\n",
        "         'Year', \n",
        "         'Month', \n",
        "         'Week', \n",
        "         'Day', \n",
        "         'Dayofweek', \n",
        "         'Dayofyear', \n",
        "    )\n",
        "    \n",
        "    for name in date_features:\n",
        "        new_df[target_prefix+name] = getattr(field.dt, name.lower())\n",
        "        \n",
        "    new_df[target_prefix+'Elapsed'] = (field - field.min()).dt.days\n",
        "    new_df[target_prefix+'MonthName'] = field.dt.month_name()\n",
        "    new_df[target_prefix+'DayName'] = field.dt.day_name()\n",
        "    new_df.drop(fieldname, axis=1, inplace=True)\n",
        "\n",
        "    return new_df\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def preprocess(fn:str) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Preprocess whatsapp text file\n",
        "    \"\"\"\n",
        "    txt_to_csv(fn)\n",
        "    chats_df = pd.read_csv(f'{fn[:-4]}.csv', parse_dates=['datetime'])\n",
        "    chats = chats_df.set_index('datetime')\n",
        "\n",
        "    chats_with_features = add_datepart(chats_df, 'datetime')\n",
        "\n",
        "    return chats, chats_with_features\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    import sys\n",
        "    if sys.argv:\n",
        "        chats_df = preprocess(sys.argv[1])\n",
        "        print(chats_df.head(5))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ngo6E1LEkrFR",
        "outputId": "f5abbd0b-5352-4143-f51e-73266f601fd1"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting utils/preprocessing.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile utils/helpers.py\n",
        "import pandas as pd\n",
        "from collections import Counter\n",
        "from wordcloud import WordCloud\n",
        "import emoji\n",
        "import altair as alt\n",
        "from urlextract import URLExtract\n",
        "import plotly.graph_objects as go\n",
        "\n",
        "urlextractor = URLExtract()\n",
        "\n",
        "def fetch_messages(df, user):\n",
        "    \"\"\"\n",
        "    Returns messages of selected user\n",
        "    \"\"\"\n",
        "    if user.lower() != 'overall':\n",
        "        df = df[df.id == user]\n",
        "\n",
        "    df = df[df.message != '<Media omitted>']\n",
        "    df = df[df.message != 'This message was deleted']\n",
        "\n",
        "    return df\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def fetch_stats(df, user):\n",
        "    \"\"\"\n",
        "    Returns stats on number of messages, members, media files, links shared\n",
        "    \"\"\"\n",
        "    if user.lower() != 'overall':\n",
        "        df = df[df.id == user]\n",
        "\n",
        "    # 1. fetch number of messages\n",
        "    num_messages = df.message.shape[0]\n",
        "\n",
        "    # 2. count number of words, and\n",
        "    # 3. number of urls\n",
        "    words = []\n",
        "    urls = []\n",
        "    for message in df.message:\n",
        "        words.extend(message)\n",
        "        urls.extend(urlextractor.find_urls(message))\n",
        "\n",
        "    num_words = len(words)\n",
        "    num_links = len(urls)\n",
        "\n",
        "    # 4. number of media files shared\n",
        "    num_medias = df[df.message == '<Media omitted>'].shape[0]\n",
        "    \n",
        "    return num_messages, num_words, num_medias, num_links\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def fetch_active_users(df):\n",
        "    \"\"\"\n",
        "    Return dataframe on most active users\n",
        "    \"\"\"\n",
        "    new_df = (df['id'].value_counts()\n",
        "                    .reset_index()\n",
        "                    .rename(columns={'index': 'User', 'id':'Messages'})\n",
        "                    .sort_values(by='Messages', ascending=False)\n",
        "    )\n",
        "\n",
        "    active_users_percent = ( (df.id.value_counts()/int(df.shape[0]) * 100)\n",
        "                                .apply(lambda x: f'{x:.2f}')\n",
        "                                .reset_index()\n",
        "                                .rename(columns={'index': 'User', 'id':'Messages(%)'})\n",
        "                                .sort_values(by='Messages(%)', ascending=False)\n",
        "                            )\n",
        "\n",
        "    return new_df, active_users_percent\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def get_wordcloud(df, user):\n",
        "    \"\"\"\n",
        "    Generates word cloud\n",
        "    \"\"\"\n",
        "    if user.lower() != 'overall':\n",
        "        df = df[df.id == user]\n",
        "\n",
        "    df = df[df.message != '<Media omitted>']\n",
        "    df = df[df.message != 'This message was deleted']\n",
        "\n",
        "    wc = WordCloud(width=700, height=300, min_font_size=12, background_color='white')\n",
        "    wc = wc.generate(df['message'].str.cat(sep=' '))\n",
        "    return wc\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def most_common_emojis(df, user, n=10):\n",
        "    \"\"\"\n",
        "    Tokenize each message, and build list of nouns, verbs, phrases \n",
        "    to return \"n\" most common words, and emojis\n",
        "    \"\"\"\n",
        "    if user.lower() != 'overall':\n",
        "        df = df[df.id == user]\n",
        "\n",
        "    df = df[df.message != '<Media omitted>']\n",
        "    df = df[df.message != 'This message was deleted']\n",
        "    \n",
        "    # tokenize\n",
        "    tokens = [word for msg in df.message for word in msg.split()]\n",
        "\n",
        "    # filter emojis\n",
        "    emojis = [word for word in tokens if word in emoji.UNICODE_EMOJI['en']]\n",
        "    common_emojis = Counter(emojis).most_common(n)\n",
        "    del emojis\n",
        " \n",
        "    # create a dataframe and build a barchart\n",
        "    def to_barchart(table):\n",
        "        df = pd.DataFrame(table)\n",
        "        df = df.rename(columns={0: 'Phrases', 1:'Count'})\n",
        "        chart = _get_barchart(df, 'Count','Phrases', 'Phrases', 'Count')\n",
        "        del df\n",
        "        return chart\n",
        "\n",
        "    return  to_barchart(common_emojis)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def _get_barchart(df, x, y, color, label):\n",
        "    \"\"\"\n",
        "    helper function to build a barchart\n",
        "    \"\"\"\n",
        "    bar_chart = alt.Chart(df).mark_bar(\n",
        "                        cornerRadiusTopLeft=3,\n",
        "                        cornerRadiusTopRight=3,\n",
        "                ).encode(\n",
        "                        x=alt.X(x, axis=alt.Axis(title=None)),\n",
        "                        y=alt.Y(y, axis=alt.Axis(title=None)),\n",
        "                        color=alt.Color(color, legend=None),\n",
        "                )\n",
        "\n",
        "    text = bar_chart.mark_text(\n",
        "                align='center',\n",
        "                dx=9,\n",
        "                color='white'\n",
        "                ).encode(\n",
        "                text=label\n",
        "            )\n",
        "    return bar_chart + text\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def timeline_stats(df, user):\n",
        "    \"\"\"\n",
        "    Return timespan of messages, first message date, and last message date\n",
        "    \"\"\"\n",
        "    if user.lower() != 'overall':\n",
        "        df = df[df.id == user]\n",
        "\n",
        "    df = df[df.message != '<Media omitted>']\n",
        "    df = df[df.message != 'This message was deleted']\n",
        "\n",
        "    if user.lower() == 'overall':\n",
        "        total_days = df.Elapsed.max() \n",
        "    else:\n",
        "        total_days = df.groupby(df.Elapsed)['message'].count().shape[0]\n",
        "\n",
        "    first_date = df.iloc[0, [7, 5, 4]].to_dict()\n",
        "    first_date = f\"{first_date['Day']}-{first_date['Month']}-{first_date['Year']}\"\n",
        "\n",
        "    last_date = df.iloc[-1, [7, 5, 4]].to_dict()\n",
        "    last_date = f\"{last_date['Day']}-{last_date['Month']}-{last_date['Year']}\"\n",
        "\n",
        "    return total_days, first_date, last_date\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def get_timelines(df, user):\n",
        "    \"\"\"\n",
        "    Build line chart to showcase yearly timeline, \n",
        "    and chart charts to show  most active months, day of week, and hour of day\n",
        "    \"\"\"\n",
        "    if user.lower() != 'overall':\n",
        "        df = df[df.id == user]\n",
        "\n",
        "    df = df[df.message != '<Media omitted>']\n",
        "    df = df[df.message != 'This message was deleted']\n",
        "\n",
        "    # timelines\n",
        "    yearly_timeline = df.resample('M')['message'].count().reset_index()\n",
        "    yearly_text = df.resample('M')['message'].apply(pd.Series.mode).tolist()\n",
        "\n",
        "    df_emojis = df['message'].apply(lambda lst:[x if x in emoji.UNICODE_EMOJI['en'] else -1 for x in lst][0])\n",
        "    df_emojis = df[df_emojis!=-1]\n",
        "\n",
        "    levels = ['datetime']\n",
        "    for idx in range(df_emojis.index.nlevels-1):\n",
        "        levels.append(idx)\n",
        "\n",
        "    df_emojis = df_emojis['message'].resample('M').apply(pd.Series.mode).reset_index(level=levels)\n",
        "\n",
        "    daily_timeline = df.resample('D')['message'].count().reset_index()\n",
        "    daily_timeline = daily_timeline[daily_timeline['message'] != 0]\n",
        "    daily_text = df.resample('D')['message'].apply(pd.Series.mode).tolist()\n",
        "\n",
        "    # most active days, and hours\n",
        "    hourly_timeline = df.groupby([df.index.hour])['message'].count().reset_index()\n",
        "    hourly_text = df.groupby([df.index.hour])['message'].apply(pd.Series.mode).reset_index()['message'].tolist()\n",
        "\n",
        "    weekly_timeline = df.groupby([df.index.day_name()])['message'].count().reset_index()\n",
        "    weekly_text = df.groupby([df.index.day_name()])['message'].apply(pd.Series.mode).reset_index()['message'].tolist()\n",
        "\n",
        "    # yearly timeline displaying total messages in each month-year\n",
        "    monthly_fig = go.Figure()\n",
        "    monthly_fig.add_trace(go.Scatter(\n",
        "        x=yearly_timeline['datetime'], \n",
        "        y=yearly_timeline['message'],\n",
        "        hovertemplate =\n",
        "        '<b>%{y:.2s} messages</b>: '+\n",
        "        '<br><i>%{text}</i>',\n",
        "        text = yearly_text,\n",
        "        name='Timeline of emoticons'\n",
        "    ))\n",
        "\n",
        "    monthly_fig.add_trace(go.Bar(\n",
        "        x=yearly_timeline['datetime'], \n",
        "        y=yearly_timeline['message'],\n",
        "        hovertemplate=\"%{y:.2s}\",\n",
        "        name='Number of Messages',\n",
        "        marker=dict(color=yearly_timeline['message'], colorbar=None),\n",
        "    ))\n",
        "\n",
        "    emoji_title = 'Timeline of messages (month-wise)' \n",
        "    if len(df_emojis) >= len(yearly_timeline):\n",
        "        emoji_title = 'Evolution of Emoticons over time'\n",
        "        \n",
        "\n",
        "    monthly_fig.update_layout(\n",
        "                        title=emoji_title,\n",
        "                        yaxis= go.layout.YAxis(title=\"Total Messages\"),\n",
        "                        showlegend=False,\n",
        "                        xaxis = go.layout.XAxis(title='Months', tickangle=45),\n",
        "                        xaxis_tickformat = '%B<br>%Y',\n",
        "                        autosize=False,\n",
        "                        height=500,\n",
        "                   )\n",
        "    \n",
        "    monthly_fig.update_xaxes(\n",
        "        rangeslider_visible=True,\n",
        "        )\n",
        "    \n",
        "\n",
        "    monthly_fig.add_trace(go.Scatter(\n",
        "        x=df_emojis['datetime'], \n",
        "        y=yearly_timeline.set_index('datetime').loc[df_emojis.datetime, 'message'],\n",
        "        text=df_emojis['message'].tolist(),\n",
        "        mode=\"markers+text\",\n",
        "        name='',\n",
        "    ))\n",
        "\n",
        "    # daily timeline of messages, displays hover text\n",
        "    daily_fig = go.Figure([\n",
        "                           go.Scatter(\n",
        "                               x=daily_timeline['datetime'], \n",
        "                               y=daily_timeline['message'],\n",
        "                               hovertemplate = \n",
        "                               '<b>%{y:.2s} messages</b>:' +\n",
        "                               '<br><i>%{text}</i>',\n",
        "                                text = daily_text,\n",
        "                                name='',\n",
        "    )])\n",
        "\n",
        "    daily_fig.update_layout(\n",
        "                        title='Timeline of messages (day-wise)',\n",
        "                        yaxis= go.layout.YAxis(title=\"Total Messages\"),\n",
        "                        showlegend=False,\n",
        "                        xaxis = go.layout.XAxis(title='Days', tickangle=45),\n",
        "                        xaxis_tickformat = '%d %B (%a)<br>%Y',\n",
        "                        autosize=False,\n",
        "                        height=500,\n",
        "                   )\n",
        "    \n",
        "    daily_fig.update_xaxes(\n",
        "        rangeslider_visible=True,\n",
        "        )\n",
        "    \n",
        "    \n",
        "    # most active days\n",
        "    weekly_fig = go.Figure([go.Bar(\n",
        "                                x=weekly_timeline['datetime'], \n",
        "                                y=weekly_timeline['message'],\n",
        "                                hovertemplate =\n",
        "                                   '<b>%{y:.2s} messages</b>: '+\n",
        "                                   '<br><i>%{text}</i>',\n",
        "                                text = weekly_text,\n",
        "                                name='',\n",
        "                                marker=dict(color=weekly_timeline['message'], colorbar=None),\n",
        "                                \n",
        "                )])\n",
        "    weekly_fig.update_layout(\n",
        "                        title='Most Active Days',\n",
        "                        yaxis= go.layout.YAxis(title=\"Total Messages\"),\n",
        "                        showlegend=False,\n",
        "                        xaxis = go.layout.XAxis(title='Day of the Week', tickangle=45)\n",
        "                   )\n",
        "    \n",
        "    weekly_fig.update_traces(texttemplate='%{y:.2s}', textposition='outside')\n",
        "\n",
        "    \n",
        "    # most active hours\n",
        "    hourly_fig = go.Figure([go.Bar(\n",
        "                                x=hourly_timeline['datetime'], \n",
        "                                y=hourly_timeline['message'],\n",
        "                                hovertemplate = \n",
        "                                    '<b>%{y:.2s} messages</b>: '+\n",
        "                                    '<br><i>%{text}</i>',\n",
        "                                text = hourly_text,\n",
        "                                name='',\n",
        "                                marker=dict(color=hourly_timeline['message'], colorbar=None),\n",
        "                                \n",
        "                )])\n",
        "    hourly_fig.update_layout(\n",
        "                        title='Most Active Hours',\n",
        "                        yaxis= go.layout.YAxis(title=\"Total Messages\"),\n",
        "                        showlegend=False,\n",
        "                        xaxis = go.layout.XAxis(title='Hours', tickangle=45, tickvals=list(range(24)))\n",
        "                   )\n",
        "    \n",
        "    hourly_fig.update_traces(texttemplate='%{y:.2s}', textposition='outside')\n",
        "\n",
        "    return  monthly_fig, daily_fig, weekly_fig, hourly_fig\n",
        "\n",
        "\n",
        "\n",
        "def get_activity_map(df, user):\n",
        "    \"\"\"\n",
        "    Plot activity map for each day and hour\n",
        "    \"\"\"\n",
        "    if user.lower() != 'overall':\n",
        "        df = df[df.id == user]\n",
        "\n",
        "    df['period'] = df['hour'].astype(str) + '-' + ((df['hour'] + 1) % 24).astype(str)\n",
        "    df = df.groupby(['DayName', 'period'])['message'].count().reset_index()\n",
        "    df = df.fillna(1)\n",
        "    # pivot = df.pivot_table(index='DayName', columns='period', values='message', aggfunc='count').fillna(0)\n",
        "    fig = alt.Chart(df).mark_rect().encode(\n",
        "            alt.X('period:O', axis=alt.Axis(title='hours')),\n",
        "            alt.Y('DayName:O', axis=alt.Axis(title='days')),\n",
        "            alt.Color('message:Q', scale=alt.Scale(scheme='goldorange'))\n",
        "        )\n",
        "    return fig"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qLmT5SX_Gk9I",
        "outputId": "5357b98c-2d6e-4de1-beda-c841f108d01b"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting utils/helpers.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile utils/topic_model.py\n",
        "from top2vec import Top2Vec\n",
        "import re\n",
        "from bs4 import BeautifulSoup\n",
        "from nltk.stem.wordnet import WordNetLemmatizer\n",
        "import nltk\n",
        "from wordcloud import WordCloud\n",
        "\n",
        "nltk.download('wordnet')\n",
        "\n",
        "global wnl\n",
        "wnl = WordNetLemmatizer()\n",
        "\n",
        "# list of custom stopwords\n",
        "stopwords= set(['br', 'the', 'i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\",\\\n",
        "            \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', \\\n",
        "            'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their',\\\n",
        "            'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', \\\n",
        "            'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', \\\n",
        "            'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', \\\n",
        "            'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after',\\\n",
        "            'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further',\\\n",
        "            'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more',\\\n",
        "            'most', 'other', 'some', 'such', 'only', 'own', 'same', 'so', 'than', 'too', 'very', \\\n",
        "            's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', \\\n",
        "            've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn',\\\n",
        "            \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn',\\\n",
        "            \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", \\\n",
        "            'won', \"won't\", 'wouldn', \"wouldn't\", 'hi', 'okay', 'ok', 'ohkay', 'bro', 'bye', 'thanks', 'thank', 'yeah', 'ya', \\\n",
        "            'u', 'ur', ])\n",
        "\n",
        "\n",
        "# https://stackoverflow.com/a/47091490/4084039\n",
        "def decontracted(phrase):\n",
        "    # specific\n",
        "    phrase = re.sub(r\"won't\", \"will not\", phrase)\n",
        "    phrase = re.sub(r\"can\\'t\", \"can not\", phrase)\n",
        "\n",
        "    # general\n",
        "    phrase = re.sub(r\"n\\'t\", \" not\", phrase)\n",
        "    phrase = re.sub(r\"\\'re\", \" are\", phrase)\n",
        "    phrase = re.sub(r\"\\'s\", \" is\", phrase)\n",
        "    phrase = re.sub(r\"\\'d\", \" would\", phrase)\n",
        "    phrase = re.sub(r\"\\'ll\", \" will\", phrase)\n",
        "    phrase = re.sub(r\"\\'t\", \" not\", phrase)\n",
        "    phrase = re.sub(r\"\\'ve\", \" have\", phrase)\n",
        "    phrase = re.sub(r\"\\'m\", \" am\", phrase)\n",
        "    return phrase\n",
        "\n",
        "\n",
        "def preprocess_text(sentence:str):\n",
        "    #a. remove html and url tags from text\n",
        "    sentence = re.sub(r\"http\\S+\", \"\", sentence)\n",
        "    sentence = BeautifulSoup(sentence, 'lxml').get_text()\n",
        "\n",
        "    #b.expand contracted terms\n",
        "    sentence = decontracted(sentence)\n",
        "\n",
        "    #c.remove non aplhabet characters\n",
        "    sentence = re.sub(\"\\S*\\d\\S*\", \"\", sentence).strip()\n",
        "    sentence = re.sub('[^A-Za-z]+', ' ', sentence)\n",
        "\n",
        "    #d. lemmatize each word in sentence\n",
        "    #e. and turn them into lower case\n",
        "    #list of stop words: https://gist.github.com/sebleier/554280\n",
        "    sentence = ' '.join(wnl.lemmatize(word.lower()) for word in sentence.\n",
        "    split() if word.lower() not in stopwords)\n",
        "\n",
        "    return sentence\n",
        "\n",
        "def get_topics(df):\n",
        "    \"\"\"\n",
        "    Preprocesses conversations to prepare it for Topic modelling\n",
        "    Returns list of wordclouds of top two topics\n",
        "    \"\"\"\n",
        "\n",
        "    df = df[df.message != '<Media omitted>']\n",
        "    df = df[df.message != 'This message was deleted']\n",
        "\n",
        "    documents = df['message'].apply(preprocess_text)\n",
        "    model = Top2Vec(documents=documents.tolist(), speed=\"learn\", workers=-1)\n",
        "\n",
        "    num_topics = model.get_num_topics()\n",
        "    if num_topics >= 2:\n",
        "        topic_words, word_scores, topic_nums = model.get_topics(2)\n",
        "\n",
        "    elif num_topics == 1:\n",
        "        topic_words, word_scores, topic_nums = model.get_topics(1)\n",
        "    \n",
        "    else:\n",
        "        return []\n",
        "\n",
        "    clouds = []\n",
        "    for topic in topic_words[:2]:\n",
        "        wc = WordCloud(width=700, height=300, min_font_size=12, background_color='white')\n",
        "        wc = wc.generate(' '.join(topic))\n",
        "        clouds.append(wc)\n",
        "\n",
        "    del model\n",
        "    del documents\n",
        "    return clouds"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nkQ3YGiK1eWn",
        "outputId": "de861e43-31b2-496f-d52e-64f01e9f9eee"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting utils/topic_model.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile app.py\n",
        "import os\n",
        "import streamlit as st\n",
        "from io import StringIO\n",
        "from utils.preprocessing import preprocess\n",
        "import altair as alt\n",
        "from matplotlib import pyplot as plt\n",
        "from numerize import numerize\n",
        "\n",
        "from utils.helpers import (\n",
        "    fetch_messages,\n",
        "    fetch_stats, \n",
        "    fetch_active_users, \n",
        "    get_wordcloud, \n",
        "    most_common_emojis,\n",
        "    get_timelines,\n",
        "    timeline_stats,\n",
        "    get_activity_map,\n",
        ")\n",
        "\n",
        "from utils.topic_model import get_topics\n",
        "\n",
        "import seaborn as sns\n",
        "sns.set()\n",
        "\n",
        "\n",
        "PAGE_CONFIG = {\"page_title\":\"App by Glad Nayak\",\"page_icon\":\":smiley:\",\"layout\":\"centered\"}\n",
        "st.set_page_config(**PAGE_CONFIG)\n",
        "\n",
        "def main():\n",
        "    \"\"\"\n",
        "    Render UI on web app, fetch and display data using utils.py\n",
        "    \"\"\"\n",
        "    st.title(\"WhatsApp Chat Analysis\")\n",
        "    uploaded_file = st.sidebar.file_uploader(\"Choose a file\")\n",
        "    if not uploaded_file:\n",
        "            st.subheader('Upload exported text file to see analysis')\n",
        "        \n",
        "    if uploaded_file:\n",
        "        try:\n",
        "            # save the file, and pass the filename to preprocess function\n",
        "            with open(os.path.join(\".\", uploaded_file.name),\"wb\") as f:\n",
        "                f.write(uploaded_file.getbuffer())\n",
        "\n",
        "            chats_with_date, chats = preprocess(uploaded_file.name)\n",
        "        \n",
        "        except:\n",
        "            st.text('Failed to read the exported file. Try again')\n",
        "            return -1\n",
        "\n",
        "        # Get all users involved in conversation\n",
        "        users = chats.id.unique().tolist()\n",
        "        users.sort()\n",
        "        users.insert(0, 'Overall')\n",
        "\n",
        "        \n",
        "        # 1. Show stats based on selected user\n",
        "        user = st.sidebar.selectbox('Show analysis wrt', users)\n",
        "\n",
        "        user_title = f'Showing Analysis for {user}' if user != 'Overall' else 'Showing Overall Analysis'\n",
        "        st.subheader(user_title)\n",
        "\n",
        "        headers = ['Members', 'Messages', 'Words', 'Media Uploaded', 'Links Shared']\n",
        "        stats = [chats.id.nunique()]\n",
        "        stats.extend(fetch_stats(chats, user))\n",
        "\n",
        "        # don't show total members for personal conversations\n",
        "        headers = headers[1:] if user != 'Overall' else headers\n",
        "        stats = stats[1:] if user != 'Overall' else stats\n",
        "\n",
        "        metrics = zip(headers, st.columns(len(headers)), stats)\n",
        "        for header, column, stat in metrics:\n",
        "            with column:\n",
        "                st.metric(label=header, value=numerize.numerize(stat))\n",
        "\n",
        "\n",
        "        # 2. display dataframe\n",
        "        st.dataframe(fetch_messages(chats, user))\n",
        "\n",
        "\n",
        "        # 3. plot activity map of user\n",
        "        title = f'Activity map of {user}' if user != 'Overall' else 'Activity Map of Users'\n",
        "        st.subheader(title)\n",
        "\n",
        "        headers = ['Active Days', 'First Message on', 'Last Message on']\n",
        "        for header, column, value in zip(headers, st.columns(3), timeline_stats(chats, user)):\n",
        "            with column:\n",
        "                st.metric(label=header, value=value)\n",
        "\n",
        "        st.altair_chart(get_activity_map(chats, user), use_container_width=True)\n",
        "\n",
        "\n",
        "        # 4. Show Overall stats\n",
        "        if user == 'Overall':\n",
        "            headers = ['Most Active', 'Most Active(%)']\n",
        "            functions = [st.bar_chart, st.table]\n",
        "            top_users, top_users_percent = fetch_active_users(chats)\n",
        "        \n",
        "            col1, col2 = st.columns(2)\n",
        "\n",
        "            # plot most active users\n",
        "            top_users = top_users[:10] \n",
        "\n",
        "            with col1:\n",
        "                st.subheader(f'Top {len(top_users)} Active Users')\n",
        "                bar_chart = alt.Chart(top_users).mark_bar(\n",
        "                        cornerRadiusTopLeft=3,\n",
        "                        cornerRadiusTopRight=3,\n",
        "                ).encode(\n",
        "                        x=alt.X('User:N', axis=alt.Axis(labelAngle=45)),\n",
        "                        y='Messages:Q',\n",
        "                        color=alt.Color('User:N', scale=alt.Scale(scheme='goldorange'), legend=None),\n",
        "                )\n",
        "\n",
        "                text = bar_chart.mark_text(\n",
        "                            align='center',\n",
        "                            dy=-5,\n",
        "                            color='white'\n",
        "                            ).encode(\n",
        "                            text='Messages'\n",
        "                        )\n",
        "                            \n",
        "                bar = (bar_chart + text).properties(height=400)\n",
        "                st.altair_chart(bar, use_container_width=True)\n",
        "\n",
        "            # plot percentage of active users\n",
        "            with col2:\n",
        "                st.subheader('Most Active Users(%)')\n",
        "                st.dataframe(top_users_percent)\n",
        "\n",
        "\n",
        "        # 5. Plot word clouds\n",
        "        try:\n",
        "            st.subheader('Word Cloud')\n",
        "            wc = get_wordcloud(chats, user)\n",
        "            fig, ax = plt.subplots()\n",
        "            ax.imshow(wc)\n",
        "            plt.axis('off')\n",
        "            st.pyplot(fig)\n",
        "\n",
        "        except:\n",
        "            st.text(\"There's been lot of silence lately...\")\n",
        "\n",
        "\n",
        "        # 6. plot metrics on words\n",
        "        header = 'Most Common Emojis'\n",
        "        table = most_common_emojis(chats, user)\n",
        "        st.subheader(header)\n",
        "        try:\n",
        "            st.altair_chart(table, use_container_width=True)\n",
        "\n",
        "        except:\n",
        "            import random\n",
        "            filler = random.choice([\n",
        "                            \"A picture is worth a thousand words, but couldn't find either\",\n",
        "                            \"Actions speak louder than words, hopefully!\"\n",
        "                ])\n",
        "            st.text(filler)\n",
        "\n",
        "\n",
        "        # 7. display timelines\n",
        "        stats = get_timelines(chats_with_date, user)\n",
        "        for timeline_plot in stats:\n",
        "            st.plotly_chart(timeline_plot, use_container_width=True)\n",
        "\n",
        "        \n",
        "        # 8. display topics\n",
        "        topics = get_topics(chats)\n",
        "        if topics:\n",
        "            st.subheader('Learning what members are talking about using Topic Modelling')\n",
        "            for idx, wc in enumerate(topics):\n",
        "                try:\n",
        "                    st.subheader(f'Topic {idx+1}')\n",
        "                    fig, ax = plt.subplots()\n",
        "                    ax.imshow(wc)\n",
        "                    plt.axis('off')\n",
        "                    st.pyplot(fig)\n",
        "\n",
        "                except:\n",
        "                    st.text(\"\")\n",
        "\n",
        "if __name__ == '__main__':\n",
        "\tmain()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PngWKBdfWmjE",
        "outputId": "d92f50eb-38c5-46e4-c6c4-3ed4033671a1"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting app.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Local Deployment using Local Tunnel - For testing"
      ],
      "metadata": {
        "id": "E48-KTmTD1lI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!streamlit run app.py --server.enableCORS=false &>/dev/null&\n",
        "\n",
        "!lt --Bypass-Tunnel-Reminder --subdomain 'bornapp' --port 8501 &>/dev/null&"
      ],
      "metadata": {
        "id": "FfX5bAWDaRmO"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# kill app and clean up memory\n",
        "st_id = !pgrep streamlit\n",
        "!kill {st_id[0]}\n",
        "\n",
        "lt_id = !pgrep lt\n",
        "!kill {lt_id[0]}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fF_Lq1J2Xmam",
        "outputId": "769c4f0a-2f65-4603-a0ae-5c90e17de4fa"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/bin/bash: line 0: kill: {lt_id[0]}: arguments must be process or job IDs\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Deploying to Heroku"
      ],
      "metadata": {
        "id": "1yBNMnZAD6Vw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# first step is to make a requirements.txt file\n",
        "#!pip freeze --local > requirements.txt\n",
        "#!pip install pipreqs\n",
        "!pipreqs --force ."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Btd8mEb00Cyf",
        "outputId": "904a2030-cde4-461a-e88c-889059f9ac00"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO: Successfully saved requirements file in ./requirements.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "setup.py and Procfile tell Heroku how to start our webapp\n",
        "<br> source: https://gilberttanner.com/blog/deploying-your-streamlit-dashboard-with-heroku"
      ],
      "metadata": {
        "id": "rugVcyLoIFpH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile setup.py\n",
        "\n",
        "mkdir -p ~/.streamlit/\n",
        "\n",
        "echo \"\\\n",
        "[general]\\n\\\n",
        "email = \\\"your-email@domain.com\\\"\\n\\\n",
        "\" > ~/.streamlit/credentials.toml\n",
        "\n",
        "echo \"\\\n",
        "[server]\\n\\\n",
        "headless = true\\n\\\n",
        "enableCORS=false\\n\\\n",
        "port = $PORT\\n\\\n",
        "\" > ~/.streamlit/config.toml"
      ],
      "metadata": {
        "id": "ELh9vWZs2Fpb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f0714da8-bca7-424e-fbb1-ea99712eae25"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing setup.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile Procfile\n",
        "\n",
        "web: sh setup.sh && streamlit run app.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1KfobRCEEk3O",
        "outputId": "ea362493-3631-4d45-a933-c196b1016cc9"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing Procfile\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# for rest of the steps, we need to be logged in to heroku, either\n",
        "# using heroku login, or heroku login -i.\n",
        "!wget https://cli-assets.heroku.com/install.sh #heroku cmd line\n",
        "!heroku login\n",
        "!git add .\n",
        "!git commit -m \"deploy to heroku\"\n",
        "!git push heroku main\n",
        "!heroku ps:scale web=1"
      ],
      "metadata": {
        "id": "XuIt913kWmJc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# runtime.txt\n",
        "!python --version"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pcl1qr_GIKMd",
        "outputId": "4d53a909-aac6-46a2-a384-6004ffa21dd8"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Python 3.7.12\n"
          ]
        }
      ]
    }
  ]
}